{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f46d82a",
   "metadata": {
    "id": "4f46d82a"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "TOv9QBbeSpuR",
   "metadata": {
    "id": "TOv9QBbeSpuR"
   },
   "source": [
    "1. Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hlMt72xjStok",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlMt72xjStok",
    "outputId": "7aef1c49-3c45-47ac-ee15-ab7f84d5a043"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q trafilatura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9900e7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T18:17:15.332053Z",
     "start_time": "2022-04-26T18:17:15.161569Z"
    },
    "id": "c9900e7d"
   },
   "source": [
    "### 1. Task: Trafilatura\n",
    "\n",
    "On https://www.voicesofyouth.org/blog young people between the ages of 13 and 24 write blog posts for Unicef about issues they care about. We will use this blog to practice crawling and extracting html data with <i>Trafilatura</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1047aa1",
   "metadata": {
    "id": "b1047aa1"
   },
   "source": [
    "**1\\. Download the html of a single blog post and extract its data into xml format. Play with the options you learned about in the session. Which parameters give you the best results? Please describe your findings in two or three sentences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d2ef29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "43d2ef29",
    "outputId": "fa49cd33-a4a0-4eaa-82cd-baec22276b49"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.output_area pre {white-space: pre;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xml.dom import minidom\n",
    "import trafilatura\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_area pre {white-space: pre;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490d3e5b",
   "metadata": {
    "id": "490d3e5b"
   },
   "outputs": [],
   "source": [
    "url = \"https://www.voicesofyouth.org/blog/export-waste-how-it-exacerbates-global-inequalities-and-counterintuitive-fight-climate-action\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "986ff177",
   "metadata": {
    "id": "986ff177"
   },
   "outputs": [],
   "source": [
    "from trafilatura.spider import focused_crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e636d91",
   "metadata": {
    "id": "2e636d91"
   },
   "outputs": [],
   "source": [
    "homepage = \"https://www.voicesofyouth.org/blog\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f994e",
   "metadata": {
    "id": "5e2f994e"
   },
   "source": [
    "<br><br>\n",
    "### 2. Task: Data analysis and visualisation\n",
    "\n",
    "In this task we will recap the most basic visualisation methods for corpus frequency distributions with the Python package <i>Plotly</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061cba4e",
   "metadata": {
    "id": "061cba4e"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9d59d19e117f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mElementTree\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_subplots\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import spacy\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ad5bf",
   "metadata": {
    "id": "dc0ad5bf"
   },
   "outputs": [],
   "source": [
    "# first we load the corpus into our notebook and transform the data into a dict data structure\n",
    "corpus = Path(\"../nexis_corpus\").rglob(\"*.xml\")\n",
    "\n",
    "articles = []\n",
    "for newspaper in corpus:\n",
    "    xml_tree = ET.parse(str(newspaper))\n",
    "    doc_elements = xml_tree.findall(\"document\")\n",
    "    \n",
    "    for doc in doc_elements:\n",
    "        articles.append(\n",
    "            {\n",
    "                \"newspaper\": doc.find(\"metadata/source\").text,\n",
    "                \"text\": \" \".join([paragraph.strip() for paragraph in doc.find(\"text\").itertext()]).strip(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "articles[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6672a51",
   "metadata": {
    "id": "b6672a51"
   },
   "source": [
    "<br/><br/>\n",
    "**1\\. Annotate the loaded corpus with spaCy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75950dd9",
   "metadata": {
    "id": "75950dd9"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619ced9",
   "metadata": {
    "id": "9619ced9"
   },
   "source": [
    "<br/><br/>\n",
    "**2\\. Now compile a frequency list and plot it as Zipf distribution as shown in the session - please refer to the <u>token</u> level. Compare your results with the distribution of lemmata (cf. the session's results). Write two sentences about the differences and discuss the applicability of each graph.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0835955e",
   "metadata": {
    "id": "0835955e"
   },
   "outputs": [],
   "source": [
    "tokens = [token for article in articles for token in article[\"tagged\"] if not token.is_space]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61febc9",
   "metadata": {
    "id": "e61febc9"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85ce9135",
   "metadata": {
    "id": "85ce9135"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
